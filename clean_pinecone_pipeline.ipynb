{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b20e4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "923e693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "\n",
    "from IPython.display import Audio\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8f763a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee import pipe # !pip install towhee towhee.models\n",
    "import laion_clap # !pip install laion-clap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2adf7e",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36008760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/juliabarnett/Desktop/Northwestern/*Fall 2023/Audio Project/git/exploring-musical-roots'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06052db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_in = 'path/to/music/files/'\n",
    "#p_in = 'toy-song-folder/mp3s/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a742c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(p_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae0fc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all our input files\n",
    "file_list = [f for f in os.listdir(p_in)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb107e",
   "metadata": {},
   "source": [
    "# CLMR Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a57a6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee import pipe, ops, DataCollection\n",
    "\n",
    "p = (\n",
    "    pipe.input('path')\n",
    "        .map('path', 'frame', ops.audio_decode.ffmpeg())\n",
    "        .map('frame', 'vecs', ops.audio_embedding.clmr())\n",
    "        .output('path', 'vecs')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db09e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dict_CLMR = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40627498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "CPU times: user 56.5 s, sys: 1.28 s, total: 57.8 s\n",
      "Wall time: 52.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# on my machine, this takes ~1 minute for 500 clips\n",
    "# results is a dictionary (audio_dict_CLMR) with key = filename, value = CLMR embedding\n",
    "string_start = p_in\n",
    "num = 0\n",
    "for file in file_list:\n",
    "    print(num)\n",
    "    # try statement here allows you to skip to items you haven't yet embedded if you stop this step midway (if a key exists, you move on to next key)\n",
    "    try:\n",
    "        audio_dict_CLMR[file]\n",
    "    except:\n",
    "        # this step grabs a CLMR embedding from the towhee CLMR pipeline\n",
    "        audio_dict_CLMR[file] = DataCollection(p(p_in + file))[0]['vecs']\n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output from here: audio_dict_CLMR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92548469",
   "metadata": {},
   "source": [
    "# CLAP Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c226fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization\n",
    "def int16_to_float32(x):\n",
    "    return (x / 32767.0).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef84fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float32_to_int16(x):\n",
    "    x = np.clip(x, a_min=-1., a_max=1.)\n",
    "    return (x * 32767.).astype(np.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394b6f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliabarnett/opt/miniconda3/envs/towhee_env/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load our best checkpoint in the paper.\n",
      "The checkpoint is already downloaded\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "model = laion_clap.CLAP_Module(enable_fusion=False)\n",
    "model.load_ckpt() # download the default pretrained checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cae6775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dict_CLAP = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af21017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "CPU times: user 3min 29s, sys: 5.25 s, total: 3min 34s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# on my machine, this takes ~3 minutes for 500 clips\n",
    "num = 0\n",
    "for file in file_list:\n",
    "    print(num)\n",
    "    # try statement here allows you to skip to items you haven't yet embedded if you stop this step midway (if a key exists, you move on to next key)\n",
    "    try:\n",
    "        audio_dict_CLAP[file]\n",
    "    except:\n",
    "        # Get audio embeddings from audio data\n",
    "        full_path = p_in + \"/\" + file\n",
    "        # this step grabs a CLAP embedding from laion_clap library\n",
    "        audio_embed = model.get_audio_embedding_from_filelist(x = [full_path], use_tensor=False)\n",
    "        audio_dict_CLAP[file] = audio_embed\n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b65ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output from here: audio_dict_CLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd14ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff6da0eb",
   "metadata": {},
   "source": [
    "# Build DataFrame of the Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "295957bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_df_CLMR = pd.DataFrame(audio_dict_CLMR.items(), columns=['vector_name','vector_embed']).reset_index()\n",
    "flat_df_CLMR.columns=['vector_id','vector_name','vector_embed']\n",
    "\n",
    "flat_df_CLAP= pd.DataFrame(audio_dict_CLAP.items(), columns=['vector_name','vector_embed']).reset_index()\n",
    "flat_df_CLAP.columns=['vector_id','vector_name','vector_embed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ab5efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do these to flatten the vectors for proper format to upload to Pinecone\n",
    "\n",
    "def flatten_vector_embed(vector_embed):\n",
    "    return list(vector_embed.flatten())\n",
    "\n",
    "def grab_song_title(vector_name):\n",
    "    return vector_name.split(\"_\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aae9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_df_CLMR[\"song_title\"] = flat_df_CLMR.apply(lambda row: grab_song_title(row['vector_name']),axis=1)\n",
    "flat_df_CLMR[\"flat_vector_embed\"] = flat_df_CLMR.apply(lambda row: flatten_vector_embed(row['vector_embed']),axis=1)\n",
    "\n",
    "flat_df_CLAP[\"song_title\"] = flat_df_CLAP.apply(lambda row: grab_song_title(row['vector_name']),axis=1)\n",
    "flat_df_CLAP[\"flat_vector_embed\"] = flat_df_CLAP.apply(lambda row: flatten_vector_embed(row['vector_embed']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73548908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "045d46f4",
   "metadata": {},
   "source": [
    "# Vector cleaning for pinecone specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8db30737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_npfloat64(original_array):\n",
    "    #return np.array(flat_df[\"flat_vector_embed\"][0],dtype=np.float64)\n",
    "    return np.array(original_array,dtype=np.float64)\n",
    "\n",
    "def convert_to_npfloat64_to_list(vector_embed_64):\n",
    "    # list(flat_df[\"flat_vector_embed_64\"][0])\n",
    "    return list(vector_embed_64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "daf9e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_df_CLMR[\"flat_vector_embed_64\"] = flat_df_CLMR.apply(lambda row: convert_to_npfloat64(row['flat_vector_embed']),axis=1)\n",
    "flat_df_CLMR[\"flat_vector_embed_64_list\"] = flat_df_CLMR.apply(lambda row: convert_to_npfloat64_to_list(row['flat_vector_embed_64']),axis=1)\n",
    "\n",
    "flat_df_CLAP[\"flat_vector_embed_64\"] = flat_df_CLAP.apply(lambda row: convert_to_npfloat64(row['flat_vector_embed']),axis=1)\n",
    "flat_df_CLAP[\"flat_vector_embed_64_list\"] = flat_df_CLAP.apply(lambda row: convert_to_npfloat64_to_list(row['flat_vector_embed_64']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191929f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "344b7d8c",
   "metadata": {},
   "source": [
    "# Build Metadata Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e259c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_folder = \"path/to/metadata/in/json/form/\"\n",
    "#metadata_folder = \"toy-song-folder/jsons/\"\n",
    "\n",
    "meta_list = [f for f in os.listdir(metadata_folder)]\n",
    "meta_list = sorted(meta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97f81f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_json(metadata_folder + \"/\" + meta_list[0])\n",
    "\n",
    "for i in range(1, len(meta_list)-1):\n",
    "    new_row = pd.read_json(metadata_folder + \"/\" + meta_list[i])\n",
    "    meta_df = meta_df.append(new_row).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1e8a5",
   "metadata": {},
   "source": [
    "### Add MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08c9d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_metadata(track_id, meta_dataframe, meta_col_interest):\n",
    "    # track_id: form = spotify:track:id_##,mp3\n",
    "    # meta_datframe: df of all the metavalues\n",
    "    # column options = album, artist_names, popularity, release_date, genre\n",
    "    df_id = track_id.split(\"_\")[0]\n",
    "    meta_row = meta_df[meta_df['uri'] == df_id].reset_index(drop=True)\n",
    "    try: \n",
    "        return meta_row[meta_col_interest][0]\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "    #return meta_row[meta_col_interest][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e20af4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>uri</th>\n",
       "      <th>album</th>\n",
       "      <th>album_uri</th>\n",
       "      <th>artist_names</th>\n",
       "      <th>artist_uris</th>\n",
       "      <th>popularity</th>\n",
       "      <th>playlist_uri</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genre</th>\n",
       "      <th>track_id</th>\n",
       "      <th>duration_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fallin' Apart - Close Counters Remix</td>\n",
       "      <td>spotify:track:02TyCVplEGRLcyW9VmzVX7</td>\n",
       "      <td>Fallin' Apart (Remixes)</td>\n",
       "      <td>spotify:album:2oxOG8dDqcJc4yJi6JCYHA</td>\n",
       "      <td>Young Franco</td>\n",
       "      <td>spotify:artist:6mK0vAO13gT8jWYANyoXAl</td>\n",
       "      <td>0</td>\n",
       "      <td>spotify:playlist:5fCodh7HOjTe1evi7OjclY</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>jazzy dnb</td>\n",
       "      <td>02TyCVplEGRLcyW9VmzVX7</td>\n",
       "      <td>197258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is not a love song</td>\n",
       "      <td>spotify:track:02WtbZhLOGOdut68sAs5aB</td>\n",
       "      <td>this is not a love song</td>\n",
       "      <td>spotify:album:0BnaolN5a5sCBUXUJIpA0d</td>\n",
       "      <td>MONA</td>\n",
       "      <td>spotify:artist:1cFDUoQ5wYnwrHsSkwvUHr</td>\n",
       "      <td>14</td>\n",
       "      <td>spotify:playlist:4rWRvJZr15hxgyZ1ozlpXb</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>korean jazz</td>\n",
       "      <td>02WtbZhLOGOdut68sAs5aB</td>\n",
       "      <td>201226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fools Rush In</td>\n",
       "      <td>spotify:track:02gauNUp4Rr9N4f1EvSx0f</td>\n",
       "      <td>Day By Day</td>\n",
       "      <td>spotify:album:0DQqEx56iwBAsSiz05KjFy</td>\n",
       "      <td>The Four Freshmen</td>\n",
       "      <td>spotify:artist:7eAF64ZiDwK2rDPSrr97D9</td>\n",
       "      <td>35</td>\n",
       "      <td>spotify:playlist:0cRFNuuqOlpXgm2ezbD69y</td>\n",
       "      <td>1994</td>\n",
       "      <td>indie jazz</td>\n",
       "      <td>02gauNUp4Rr9N4f1EvSx0f</td>\n",
       "      <td>156159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We Have All The Time In The World (feat. The D...</td>\n",
       "      <td>spotify:track:03qBsFKs6aD6SFk8RqpHm9</td>\n",
       "      <td>Pisnicky, ktere Karel miloval</td>\n",
       "      <td>spotify:album:7DtH75CC7NpoenTvu8A0Av</td>\n",
       "      <td>Various Artists</td>\n",
       "      <td>spotify:artist:0LyfQWJT6nXafLPZqxe9Of</td>\n",
       "      <td>34</td>\n",
       "      <td>spotify:playlist:37i9dQZF1DWT8pEFXwG9EZ</td>\n",
       "      <td>2012-12-14</td>\n",
       "      <td>bossa nova jazz</td>\n",
       "      <td>03qBsFKs6aD6SFk8RqpHm9</td>\n",
       "      <td>193973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It Could Happen To You</td>\n",
       "      <td>spotify:track:04edq8nmRKQOo2yQLBwgk7</td>\n",
       "      <td>Waltz For Debby</td>\n",
       "      <td>spotify:album:5jMKNvYYHH08quqLK3ylTC</td>\n",
       "      <td>Monica Zetterlund</td>\n",
       "      <td>spotify:artist:7mvvG63CNSY93JWAJ37rnD</td>\n",
       "      <td>33</td>\n",
       "      <td>spotify:playlist:37i9dQZF1DX1C8KR4UJlnr</td>\n",
       "      <td>1964-12-01</td>\n",
       "      <td>trad jazz catala</td>\n",
       "      <td>04edq8nmRKQOo2yQLBwgk7</td>\n",
       "      <td>179666.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0               Fallin' Apart - Close Counters Remix   \n",
       "1                            this is not a love song   \n",
       "2                                      Fools Rush In   \n",
       "3  We Have All The Time In The World (feat. The D...   \n",
       "4                             It Could Happen To You   \n",
       "\n",
       "                                    uri                          album  \\\n",
       "0  spotify:track:02TyCVplEGRLcyW9VmzVX7        Fallin' Apart (Remixes)   \n",
       "1  spotify:track:02WtbZhLOGOdut68sAs5aB        this is not a love song   \n",
       "2  spotify:track:02gauNUp4Rr9N4f1EvSx0f                     Day By Day   \n",
       "3  spotify:track:03qBsFKs6aD6SFk8RqpHm9  Pisnicky, ktere Karel miloval   \n",
       "4  spotify:track:04edq8nmRKQOo2yQLBwgk7                Waltz For Debby   \n",
       "\n",
       "                              album_uri       artist_names  \\\n",
       "0  spotify:album:2oxOG8dDqcJc4yJi6JCYHA       Young Franco   \n",
       "1  spotify:album:0BnaolN5a5sCBUXUJIpA0d               MONA   \n",
       "2  spotify:album:0DQqEx56iwBAsSiz05KjFy  The Four Freshmen   \n",
       "3  spotify:album:7DtH75CC7NpoenTvu8A0Av    Various Artists   \n",
       "4  spotify:album:5jMKNvYYHH08quqLK3ylTC  Monica Zetterlund   \n",
       "\n",
       "                             artist_uris  popularity  \\\n",
       "0  spotify:artist:6mK0vAO13gT8jWYANyoXAl           0   \n",
       "1  spotify:artist:1cFDUoQ5wYnwrHsSkwvUHr          14   \n",
       "2  spotify:artist:7eAF64ZiDwK2rDPSrr97D9          35   \n",
       "3  spotify:artist:0LyfQWJT6nXafLPZqxe9Of          34   \n",
       "4  spotify:artist:7mvvG63CNSY93JWAJ37rnD          33   \n",
       "\n",
       "                              playlist_uri release_date             genre  \\\n",
       "0  spotify:playlist:5fCodh7HOjTe1evi7OjclY   2021-01-15         jazzy dnb   \n",
       "1  spotify:playlist:4rWRvJZr15hxgyZ1ozlpXb   2022-11-04       korean jazz   \n",
       "2  spotify:playlist:0cRFNuuqOlpXgm2ezbD69y         1994        indie jazz   \n",
       "3  spotify:playlist:37i9dQZF1DWT8pEFXwG9EZ   2012-12-14   bossa nova jazz   \n",
       "4  spotify:playlist:37i9dQZF1DX1C8KR4UJlnr   1964-12-01  trad jazz catala   \n",
       "\n",
       "                 track_id  duration_ms  \n",
       "0  02TyCVplEGRLcyW9VmzVX7     197258.0  \n",
       "1  02WtbZhLOGOdut68sAs5aB     201226.0  \n",
       "2  02gauNUp4Rr9N4f1EvSx0f     156159.0  \n",
       "3  03qBsFKs6aD6SFk8RqpHm9     193973.0  \n",
       "4  04edq8nmRKQOo2yQLBwgk7     179666.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "178f9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_df_CLMR[\"genre\"] = flat_df_CLMR.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'genre'),axis=1)\n",
    "flat_df_CLMR[\"album\"] = flat_df_CLMR.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'album'),axis=1)\n",
    "flat_df_CLMR[\"name\"] = flat_df_CLMR.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'name'),axis=1)\n",
    "flat_df_CLMR[\"artist\"] = flat_df_CLMR.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'artist_names'),axis=1)\n",
    "\n",
    "flat_df_CLAP[\"genre\"] = flat_df_CLAP.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'genre'),axis=1)\n",
    "flat_df_CLAP[\"album\"] = flat_df_CLAP.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'album'),axis=1)\n",
    "flat_df_CLAP[\"name\"] = flat_df_CLAP.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'name'),axis=1)\n",
    "flat_df_CLAP[\"artist\"] = flat_df_CLAP.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'artist_names'),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a9692867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_year_from_date(full_date):\n",
    "    if type(full_date) == int:\n",
    "        return str(full_date)\n",
    "    else:\n",
    "        try:\n",
    "            return full_date[:4]\n",
    "        except:\n",
    "            return \"CHECK_THIS\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9af55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[\"year\"] = meta_df.apply(lambda row: strip_year_from_date(row['release_date']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9157f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_df_CLMR[\"year\"] = flat_df_CLMR.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'year'),axis=1)\n",
    "flat_df_CLAP[\"year\"] = flat_df_CLAP.apply(lambda row: look_up_metadata(row['vector_name'], meta_df, 'year'),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b362841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_vector_clip(vector_name):\n",
    "    return vector_name.split(\".\")[0].split(\"_\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2aa813b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_df_CLMR[\"vector_clip_num\"] = flat_df_CLMR.apply(lambda row: strip_vector_clip(row['vector_name']),axis=1)\n",
    "flat_df_CLAP[\"vector_clip_num\"] = flat_df_CLAP.apply(lambda row: strip_vector_clip(row['vector_name']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1408996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_num(vector_name_str):\n",
    "    return str(int(vector_name_str.split(\"_\")[2].split(\".\")[0]) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bad98e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_df_CLMR['embedding_triplet_num'] = flat_df_CLMR.vector_name.apply(get_triplet_num)\n",
    "flat_df_CLAP['embedding_triplet_num'] = flat_df_CLAP.vector_name.apply(get_triplet_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e65b8579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique songs: 493\n",
      "unique songs: 493\n"
     ]
    }
   ],
   "source": [
    "print(\"unique songs:\", len(flat_df_CLMR.song_title.unique()))\n",
    "print(\"unique songs:\", len(flat_df_CLAP.song_title.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9d6d6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is for later to check against metadata once uploaded\n",
    "\n",
    "# def index_meta_search(vector_embed, index_var):\n",
    "#     test_quer = index_var.query(\n",
    "#     vector=vector_embed,\n",
    "#     top_k=10,\n",
    "#     #include_values=False,\n",
    "#       include_metadata=True\n",
    "#     )\n",
    "#     return test_quer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946bc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "986a126b",
   "metadata": {},
   "source": [
    "# Pinecone Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f43d1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from typing import List, Iterator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#import wget\n",
    "from ast import literal_eval\n",
    "\n",
    "# # Pinecone's client library for Python\n",
    "import pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "47d8bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to upload your vectors\n",
    "# Models a simple batch generator that make chunks out of an input DataFrame\n",
    "class BatchGenerator:\n",
    "    #\n",
    "    def __init__(self, batch_size: int = 10) -> None:\n",
    "        self.batch_size = batch_size\n",
    "    #\n",
    "    # Makes chunks out of an input DataFrame\n",
    "    def to_batches(self, df: pd.DataFrame) -> Iterator[pd.DataFrame]:\n",
    "        splits = self.splits_num(df.shape[0])\n",
    "        if splits <= 1:\n",
    "            yield df\n",
    "        else:\n",
    "            for chunk in np.array_split(df, splits):\n",
    "                yield chunk\n",
    "    #\n",
    "    # Determines how many chunks DataFrame contains\n",
    "    def splits_num(self, elements: int) -> int:\n",
    "        return round(elements / self.batch_size)\n",
    "    #\n",
    "    __call__ = to_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c04a286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batcher = BatchGenerator(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7278c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can sign up for free here: https://www.pinecone.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e98625a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"XXXXXX\" # put your API key from pinecone here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99a6f9",
   "metadata": {},
   "source": [
    "# CLMR Uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "35b7c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is free! so we can leave this one running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fcb39044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the index (must match that on Pinecone website)\n",
    "# CLMR:\n",
    "index_name_clmr = 'name-of-clmr-pinecone-index' # free (comes with plan, can have 100k records)\n",
    "index_env_clmr = 'name-of-pinecone-environment' # free (comes with plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9286c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=api_key, \n",
    "              environment=index_env_clmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0f95e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_clmr = pinecone.Index(index_name=index_name_clmr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2c2936c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clmr-small-index']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.list_indexes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d652781",
   "metadata": {},
   "source": [
    "### Upload vectors to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "200282a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n"
     ]
    }
   ],
   "source": [
    "batch_id = 0\n",
    "for batch_df in df_batcher(flat_df_CLMR):\n",
    "    #print(batch_df)\n",
    "    print(\"batch:\", batch_id)\n",
    "    batch_id = batch_id + 1\n",
    "    index_clmr.upsert(vectors=list(zip(batch_df[\"vector_name\"],batch_df[\"flat_vector_embed_64_list\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a0489",
   "metadata": {},
   "source": [
    "### Upload their metadata as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "027cfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_list_update_metadata_CLMR = []\n",
    "# it sometimes fails due to weird characters in metadata, so I have a try + catch wrapper to allow it to continue and notify us of weird characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f90394cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "CPU times: user 2.07 s, sys: 127 ms, total: 2.19 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# this takes about ~1 minute\n",
    "# for vec_id in range(0, 5): # I always test it's working on 5 before jumping into the whole list\n",
    "for vec_id in range(5,len(flat_df_CLMR)):\n",
    "    try:\n",
    "        print(vec_id)\n",
    "        row = flat_df_CLMR.iloc[vec_id]\n",
    "        index_clmr.update(id=str(row['vector_name']),\n",
    "                        set_metadata={\"genre\": row['genre'],\n",
    "                                      \"song\" : row['name'],\n",
    "                                      \"album\": row['album'],\n",
    "                                      \"artists\": row['artist'],\n",
    "                                      \"year\" : str(row['year']),\n",
    "                                      \"clip_num\" : row['vector_clip_num'],\n",
    "                                      \"triplet_num\": str(row['embedding_triplet_num']),\n",
    "                                      \"spotify_id\" : row['song_title']\n",
    "                                  })\n",
    "    except:\n",
    "        print(\"failed on:\", vec_id)\n",
    "        failed_list_update_metadata_CLMR.append(vec_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ad717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22de8fad",
   "metadata": {},
   "source": [
    "# CLAP Uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the index (must match that on Pinecone website)\n",
    "# CLAP:\n",
    "index_name_clap = 'clap-small-index'  # costs money (only activate if need be, then collapse to collection)\n",
    "index_env_clap = 'us-west1-gcp'  # costs money (only activate if need be, then collapse to collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b82855be",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_clap = pinecone.Index(index_name=index_name_clap)\n",
    "\n",
    "pinecone.init(api_key=api_key, \n",
    "              environment=index_env_clap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "44407067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.index.Index at 0x7fa180085110>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_clap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "43df0b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clap-small-index']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c5aa3",
   "metadata": {},
   "source": [
    "### Upload vectors to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c3471b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n"
     ]
    }
   ],
   "source": [
    "batch_id = 0\n",
    "for batch_df in df_batcher(flat_df_CLAP):\n",
    "    #print(batch_df)\n",
    "    print(\"batch:\", batch_id)\n",
    "    batch_id = batch_id + 1\n",
    "    index_clap.upsert(vectors=list(zip(batch_df[\"vector_name\"],batch_df[\"flat_vector_embed_64_list\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1bdab",
   "metadata": {},
   "source": [
    "### Upload their metadata as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "982bd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_list_update_metadata_CLAP = []\n",
    "# it sometimes fails due to weird characters in metadata, so I have a try + catch wrapper to allow it to continue and notify us of weird characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a544c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "CPU times: user 2.4 s, sys: 174 ms, total: 2.58 s\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# this takes about ~1 minute\n",
    "# for vec_id in range(0, 5): # I always test it's working on 5 before jumping into the whole list\n",
    "for vec_id in range(5,len(flat_df_CLAP)):\n",
    "    try:\n",
    "        print(vec_id)\n",
    "        row = flat_df_CLAP.iloc[vec_id]\n",
    "        index_clap.update(id=str(row['vector_name']),\n",
    "                        set_metadata={\"genre\": row['genre'],\n",
    "                                      \"song\" : row['name'],\n",
    "                                      \"album\": row['album'],\n",
    "                                      \"artists\": row['artist'],\n",
    "                                      \"year\" : str(row['year']),\n",
    "                                      \"clip_num\" : row['vector_clip_num'],\n",
    "                                      \"triplet_num\": str(row['embedding_triplet_num']),\n",
    "                                      \"spotify_id\" : row['song_title']\n",
    "                                  })\n",
    "    except:\n",
    "        print(\"failed on:\", vec_id)\n",
    "        failed_list_update_metadata_CLAP.append(vec_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baacb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e4a8fd",
   "metadata": {},
   "source": [
    "# Now everything is uploaded! Play around with similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8c645732",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_in_ood = 'data/nathan_500_clips/not_uploaded/music/'\n",
    "# find all our out of domain input files\n",
    "file_list_ood = [f for f in os.listdir(p_in_ood)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a28d112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = file_list_ood[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "09857830",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clmr_embed = DataCollection(p(p_in_ood + test_file))[0]['vecs']\n",
    "test_clap_embed = model.get_audio_embedding_from_filelist(x = [p_in_ood + \"/\" + test_file], use_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4553f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_clmr_embed = convert_to_npfloat64_to_list(convert_to_npfloat64(flatten_vector_embed(test_clmr_embed)))\n",
    "clean_test_clap_embed = convert_to_npfloat64_to_list(convert_to_npfloat64(flatten_vector_embed(test_clap_embed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "76072615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'spotify:track:5ME6skzJ9tCFJQttEvhHjC_8_0.mp3',\n",
       "              'metadata': {'album': \"Routes To Django & Bireli Swing '81\",\n",
       "                           'artists': 'Bireli Lagrene Ensemble',\n",
       "                           'clip_num': '8',\n",
       "                           'genre': 'gypsy jazz',\n",
       "                           'song': \"I've Found A New Baby\",\n",
       "                           'spotify_id': 'spotify:track:5ME6skzJ9tCFJQttEvhHjC',\n",
       "                           'triplet_num': '1',\n",
       "                           'year': '1998'},\n",
       "              'score': 0.720589817,\n",
       "              'values': []},\n",
       "             {'id': 'spotify:track:59XAGzfCOHxdwzjmXpJrag_2_2.mp3',\n",
       "              'metadata': {'album': 'Goodies',\n",
       "                           'artists': 'The Howard Roberts Quartet',\n",
       "                           'clip_num': '2',\n",
       "                           'genre': 'jazz quartet',\n",
       "                           'song': 'Girl Talk',\n",
       "                           'spotify_id': 'spotify:track:59XAGzfCOHxdwzjmXpJrag',\n",
       "                           'triplet_num': '3',\n",
       "                           'year': '1965'},\n",
       "              'score': 0.674950719,\n",
       "              'values': []},\n",
       "             {'id': 'spotify:track:0h3wot8Yt2SKK6Nw170b0M_2_1.mp3',\n",
       "              'metadata': {'album': 'Pannonica',\n",
       "                           'artists': 'Battista Lena',\n",
       "                           'clip_num': '2',\n",
       "                           'genre': 'dinner jazz',\n",
       "                           'song': 'Pannonica',\n",
       "                           'spotify_id': 'spotify:track:0h3wot8Yt2SKK6Nw170b0M',\n",
       "                           'triplet_num': '2',\n",
       "                           'year': '2022'},\n",
       "              'score': 0.673358738,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'readUnits': 6}}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_clmr.query(\n",
    "  vector=clean_test_clmr_embed,\n",
    "  top_k=3,\n",
    "  #include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cf28c843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'spotify:track:0OtLnXYlL9nvUDhD5CCVaY_21_2.mp3',\n",
       "              'metadata': {'album': 'Hoping for Love',\n",
       "                           'artists': 'Isabelle Antena',\n",
       "                           'clip_num': '21',\n",
       "                           'genre': 'belgian modern jazz',\n",
       "                           'song': 'Le Poisson Des Mers Du Sud',\n",
       "                           'spotify_id': 'spotify:track:0OtLnXYlL9nvUDhD5CCVaY',\n",
       "                           'triplet_num': '3',\n",
       "                           'year': '1987'},\n",
       "              'score': 0.86289078,\n",
       "              'values': []},\n",
       "             {'id': 'spotify:track:4H39zVSTrnXomaInIgfpDY_7_0.mp3',\n",
       "              'metadata': {'album': 'Relaxing Bossa Lounge 3',\n",
       "                           'artists': 'Various Artists',\n",
       "                           'clip_num': '7',\n",
       "                           'genre': 'brazilian modern jazz',\n",
       "                           'song': 'You Give Me Something (Bossa Version)',\n",
       "                           'spotify_id': 'spotify:track:4H39zVSTrnXomaInIgfpDY',\n",
       "                           'triplet_num': '1',\n",
       "                           'year': '2010'},\n",
       "              'score': 0.820139587,\n",
       "              'values': []},\n",
       "             {'id': 'spotify:track:5HBDU0kn4f4hkZMjABsltT_4_1.mp3',\n",
       "              'metadata': {'album': 'Young Modern',\n",
       "                           'artists': 'Silverchair',\n",
       "                           'clip_num': '4',\n",
       "                           'genre': 'australian blues',\n",
       "                           'song': 'Straight Lines',\n",
       "                           'spotify_id': 'spotify:track:5HBDU0kn4f4hkZMjABsltT',\n",
       "                           'triplet_num': '2',\n",
       "                           'year': '2007'},\n",
       "              'score': 0.767478466,\n",
       "              'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_clap.query(\n",
    "  vector=clean_test_clap_embed,\n",
    "  top_k=3,\n",
    "  #include_values=False,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27860a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf39e796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (towhee_env)",
   "language": "python",
   "name": "towhee_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
